{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e402f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# public libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scanpy as sc\n",
    "sc.settings.set_figure_params(dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29137aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace7b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "## DEFINE CONSTANTS\n",
    "stretch_length_cdr3 = 30\n",
    "stretch_length_ag = 15\n",
    "\n",
    "## DEFINE ENCODERS\n",
    "def _encode_cdr3s(peptide):\n",
    "    # define columns\n",
    "    columns = cdr3_alphabet+['charge','hydrophobicity','weight','sulfur','aromatic']\n",
    "    columns_returned = [f'{idx}{col}' for idx in range(stretch_length_cdr3) for col in columns]\n",
    "    # create the tracking dataframe\n",
    "    X = pd.DataFrame(np.nan, index=range(stretch_length_cdr3), columns=columns)\n",
    "    step = stretch_length_cdr3 / len(peptide)\n",
    "    for idx, aa in enumerate(peptide):\n",
    "        # find start and end of each peptide\n",
    "        start, end = round(idx * step), round((idx + 1) * step)\n",
    "        if idx == len(peptide) - 1:\n",
    "            end = stretch_length_cdr3\n",
    "        # map to the coordinates\n",
    "        charge = amino_acid_charge[aa]\n",
    "        hydrophobicity = amino_acid_hydrophobicity[aa] / 2\n",
    "        weight = amino_acid_weights[aa] / stretch_length_cdr3\n",
    "        sulfur = 1 * (aa in has_sulfur)\n",
    "        aromatic = 1 * (aa in is_aromatic)\n",
    "        X.loc[start:end, cdr3_alphabet] = 0\n",
    "        X.loc[start:end, aa] = 1\n",
    "        X.loc[start:end, ['charge','hydrophobicity','weight','sulfur','aromatic']] = charge, hydrophobicity, weight, sulfur, aromatic\n",
    "    assert not X.isna().any().any()\n",
    "    return pd.Series(X.values.flatten(), index=columns_returned, name=peptide)\n",
    "def encode_cdr3s(cdr3s, n_cpus, verbose):\n",
    "    if verbose: print('\\n\\tencoding TCRs...', end='')\n",
    "    # the first step is to create a conversion map for CDR3 sequences\n",
    "    global cdr3_alphabet\n",
    "    cdr3_alphabet = sorted(set([el for x in cdr3s for el in list(x)]))\n",
    "    # work through cdr3s\n",
    "    els = []\n",
    "    with mp.Pool(n_cpus) as pool:\n",
    "        for el in pool.imap_unordered(_encode_cdr3s, cdr3s):\n",
    "            els.append(el)\n",
    "    cdr3_to_X = pd.concat(els, axis=1).T\n",
    "    if verbose: print('done!')\n",
    "    return cdr3_to_X\n",
    "\n",
    "\n",
    "def _encode_ags(peptide):\n",
    "    # define columns\n",
    "    columns = ag_alphabet+['charge','hydrophobicity','weight','sulfur','aromatic']\n",
    "    columns_returned = [f'{idx}{col}' for idx in range(stretch_length_ag) for col in columns]\n",
    "    # create the tracking dataframe\n",
    "    X = pd.DataFrame(np.nan, index=range(stretch_length_ag), columns=columns)\n",
    "    step = stretch_length_ag / len(peptide)\n",
    "    for idx, aa in enumerate(peptide):\n",
    "        # find start and end of each peptide\n",
    "        start, end = round(idx * step), round((idx + 1) * step)\n",
    "        if idx == len(peptide) - 1:\n",
    "            end = stretch_length_ag\n",
    "        # map to the coordinates\n",
    "        charge = amino_acid_charge[aa]\n",
    "        hydrophobicity = amino_acid_hydrophobicity[aa] / 2\n",
    "        weight = amino_acid_weights[aa] / stretch_length_ag\n",
    "        sulfur = 1 * (aa in has_sulfur)\n",
    "        aromatic = 1 * (aa in is_aromatic)\n",
    "        X.loc[start:end, ag_alphabet] = 0\n",
    "        X.loc[start:end, aa] = 1\n",
    "        X.loc[start:end, ['charge','hydrophobicity','weight','sulfur','aromatic']] = charge, hydrophobicity, weight, sulfur, aromatic\n",
    "    assert not X.isna().any().any()\n",
    "    return pd.Series(X.values.flatten(), index=columns_returned, name=peptide)\n",
    "def encode_ags(ags, n_cpus, verbose):\n",
    "    if verbose: print('\\tencoding antigens...', end='')\n",
    "    # the first step is to create a conversion map for ag sequences\n",
    "    global ag_alphabet\n",
    "    ag_alphabet = sorted(set([el for x in ags for el in list(x)]))\n",
    "    # work through antigens\n",
    "    els = []\n",
    "    with mp.Pool(n_cpus) as pool:\n",
    "        for el in pool.imap_unordered(_encode_ags, ags):\n",
    "            els.append(el)\n",
    "    ag_to_X = pd.concat(els, axis=1).T\n",
    "    if verbose: print('done!')\n",
    "    return ag_to_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750df001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll read in the train and test\n",
    "train = pd.read_csv('../external/tcr-specificity-prediction-challenge/VDJdb_paired_chain.csv')\n",
    "test = pd.read_csv('../external/tcr-specificity-prediction-challenge/test.csv')\n",
    "# retrieve encodings\n",
    "cdr3_to_X = encode_cdr3s(pd.concat([train, test], axis=0)['CDR3b_extended'].unique(), 40, False)\n",
    "ag_to_X = encode_cdr3s(pd.concat([train, test], axis=0)['Peptide'].unique(), 40, False)\n",
    "# get the formattings to match\n",
    "train['HLA'] = train['HLA'].str.replace('HLA-','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c27fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the originals\n",
    "train_orig, test_orig = train.copy(), test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef4fc3d",
   "metadata": {},
   "source": [
    "## tcrb only v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df4a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the levenshtein distance for the epitope information\n",
    "from Levenshtein import distance\n",
    "from tqdm import tqdm\n",
    "# our goal here is to find the similar and different peptides in our dataset\n",
    "counts = train['Peptide'].value_counts()\n",
    "train_l = pd.DataFrame(index=counts.index, columns=counts.index)\n",
    "for idx, pep1 in tqdm(enumerate(counts.index[:-1]), total=train_l.shape[0]):\n",
    "    train_l.loc[pep1, pep1] = 0\n",
    "    for pep2 in counts.index[idx+1:]:\n",
    "        d = distance(pep1, pep2)\n",
    "        train_l.loc[pep1, pep2] = d\n",
    "        train_l.loc[pep2, pep1] = d\n",
    "train_l.loc[pep2, pep2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad5116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve hla specific predictions\n",
    "hla2pred = {}\n",
    "for hla in test_orig['HLA'].unique():\n",
    "    # subset for HLA\n",
    "    train = train_orig.loc[train_orig['HLA'] == hla].copy()\n",
    "    test = test_orig.loc[test_orig['HLA'] == hla].copy()\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    # set seed and identify irrelevant matches\n",
    "    np.random.seed(0)\n",
    "    irrs = []\n",
    "\n",
    "    # re count the peptides for this HLA\n",
    "    counts = train['Peptide'].value_counts()\n",
    "    for pep in tqdm(counts.index):\n",
    "        # gather peptide cdr3 information\n",
    "        n_cdr3s = counts.loc[pep]\n",
    "        pep_cdr3s = train.loc[train['Peptide'] == pep, 'CDR3b_extended'].unique()\n",
    "        # we first grab the peptide levenshtein distances\n",
    "        pep_levenshtein = train_l[pep].sort_values()[::-1]\n",
    "        # systematically identify negative controls\n",
    "        irr_cdr3s = []\n",
    "        while n_cdr3s > 0:\n",
    "            # then we look at the max distance, gather those peptides, randomly choose one\n",
    "            # find associated CDR3s that don't overlap with the current peptide\n",
    "            vmax = pep_levenshtein.max()\n",
    "            irr_peps = pep_levenshtein.index[pep_levenshtein == vmax]\n",
    "            # reset for the next round if needed\n",
    "            pep_levenshtein = pep_levenshtein[pep_levenshtein < vmax]\n",
    "            # find the irrelevant peptide CDR3s and make sure they don't overlap\n",
    "            mask = (train_orig['Peptide'].isin(irr_peps)) & (~train_orig['CDR3b_extended'].isin(pep_cdr3s))\n",
    "            if sum(mask) == 0:\n",
    "                continue\n",
    "            # if we have cdr3s then grab them out\n",
    "            cdr3s = train_orig.loc[mask, 'CDR3b_extended']\n",
    "            # if there are more than we need randomly select\n",
    "            if len(cdr3s) > n_cdr3s:\n",
    "                irr_cdr3s += np.random.choice(cdr3s, size=n_cdr3s, replace=False).tolist()\n",
    "                break\n",
    "            # otherwise keep moving\n",
    "            else:\n",
    "                irr_cdr3s += cdr3s.tolist()\n",
    "                n_cdr3s -= len(cdr3s)\n",
    "        # compile the full list\n",
    "        irr = pd.DataFrame(irr_cdr3s, columns=['CDR3b_extended'])\n",
    "        irr['Peptide'] = pep\n",
    "        irrs.append(irr)\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "    ## CONVERT TO CORRECT FORMAT\n",
    "    # create X for training\n",
    "    X_train = pd.concat([pd.concat(irrs, axis=0), train], axis=0)\n",
    "    X_train_cdr3s = cdr3_to_X.loc[X_train['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_train_epitopes = ag_to_X.loc[X_train['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_train_cdr3s.columns = 'cdr3:' + X_train_cdr3s.columns\n",
    "    X_train_epitopes.columns = 'ag:' + X_train_epitopes.columns\n",
    "    X_train = X_train_cdr3s.join(X_train_epitopes)\n",
    "\n",
    "    # grab y for training\n",
    "    y_train = pd.Series(0, index=X_train.index)\n",
    "    y_train.iloc[-train.shape[0]:] = 1\n",
    "\n",
    "    # confirm the same length\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "    # create X for testing\n",
    "    X_test_cdr3s = cdr3_to_X.loc[test['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_test_epitopes = ag_to_X.loc[test['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_test_cdr3s.columns = 'cdr3:' + X_test_cdr3s.columns\n",
    "    X_test_epitopes.columns = 'ag:' + X_test_epitopes.columns\n",
    "    X_test = X_test_cdr3s.join(X_test_epitopes)\n",
    "\n",
    "    # remove constant columns\n",
    "    X_train = X_train.loc[:, X_train.nunique(0) > 1]\n",
    "    X_train = X_train.loc[:, X_train.sum(0) > 0]\n",
    "    # read in normalization factors\n",
    "    means = X_train.mean(0)\n",
    "    stds = (X_train - means).std(0)\n",
    "    # subset for relevant columns\n",
    "    X_train = X_train[means.index]\n",
    "    X_test = X_test[means.index]\n",
    "    # normalize\n",
    "    X_train -= means\n",
    "    X_train /= stds\n",
    "    X_test -= means\n",
    "    X_test /= stds\n",
    "\n",
    "\n",
    "    ## SETUP MODEL\n",
    "    # retrieve the appropriate columns\n",
    "    cols_cdr3 = X_train.columns[X_train.columns.str.startswith('cdr3')]\n",
    "    cols_ag = X_train.columns[X_train.columns.str.startswith('ag')]\n",
    "\n",
    "    # determine model parameters\n",
    "    # > layer for cdr3 alone\n",
    "    input_1 = keras.layers.Input(shape=(len(cols_cdr3)))\n",
    "    output_1 = keras.layers.Dense(200, activation='sigmoid')(input_1)\n",
    "    # > layer for ag alone\n",
    "    input_2 = keras.layers.Input(shape=(len(cols_ag)))\n",
    "    output_2 = keras.layers.Dense(100, activation='sigmoid')(input_2)\n",
    "    # > combined layer\n",
    "    concat_3 = keras.layers.Concatenate()([output_1, output_2])\n",
    "    output_3 = keras.layers.Dense(100, activation='sigmoid')(concat_3)\n",
    "    # > final logit softmax layer\n",
    "    output_4 = keras.layers.Dense(1, activation='sigmoid')(output_3)\n",
    "    model = keras.Model(inputs=[input_1, input_2], outputs=[output_4])\n",
    "    # set up the training parameters for the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC'],\n",
    "    )\n",
    "    # train the model\n",
    "    history = model.fit([X_train[cols_cdr3], X_train[cols_ag]], y_train,\n",
    "                        epochs=10,\n",
    "                        validation_data=([X_train[cols_cdr3], X_train[cols_ag]], y_train),\n",
    "                        workers=40, use_multiprocessing=True)\n",
    "\n",
    "    # testing predictions\n",
    "    y_pred = model.predict([X_test[cols_cdr3], X_test[cols_ag]],\n",
    "                           workers=40, use_multiprocessing=True)[:, 0]\n",
    "    y_pred = pd.Series(y_pred, index=test.index, name='Pred%')\n",
    "    hla2pred[hla] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9da47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back in predictions\n",
    "test = test_orig.copy()\n",
    "test['Prediction'] = np.nan\n",
    "for pred in hla2pred.values():\n",
    "    test.loc[pred.index, 'Prediction'] = pred\n",
    "# write down predictions\n",
    "test.to_csv('../external/tcr-specificity-prediction-challenge/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552dabff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e345b4c2",
   "metadata": {},
   "source": [
    "## tcrb only v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361bf9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve hla specific predictions\n",
    "hla2pred = {}\n",
    "for hla in test_orig['HLA'].unique():\n",
    "    # subset for HLA\n",
    "    train = train_orig.loc[train_orig['HLA'] == hla].copy()\n",
    "    test = test_orig.loc[test_orig['HLA'] == hla].copy()\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    # set seed and identify irrelevant matches\n",
    "    np.random.seed(0)\n",
    "    irrs = []\n",
    "\n",
    "    # re count the peptides for this HLA\n",
    "    counts = train['Peptide'].value_counts()\n",
    "    for pep in tqdm(counts.index):\n",
    "        # gather peptide cdr3 information\n",
    "        n_cdr3s = counts.loc[pep]\n",
    "        pep_cdr3s = train.loc[train['Peptide'] == pep, 'CDR3b_extended'].unique()\n",
    "        # we first grab the peptide levenshtein distances\n",
    "        pep_levenshtein = train_l[pep].sort_values()[::-1]\n",
    "        # systematically identify negative controls\n",
    "        irr_cdr3s = []\n",
    "        while n_cdr3s > 0:\n",
    "            # then we look at the max distance, gather those peptides, randomly choose one\n",
    "            # find associated CDR3s that don't overlap with the current peptide\n",
    "            vmax = pep_levenshtein.max()\n",
    "            irr_peps = pep_levenshtein.index[pep_levenshtein == vmax]\n",
    "            # reset for the next round if needed\n",
    "            pep_levenshtein = pep_levenshtein[pep_levenshtein < vmax]\n",
    "            # find the irrelevant peptide CDR3s and make sure they don't overlap\n",
    "            mask = (train_orig['Peptide'].isin(irr_peps)) & (~train_orig['CDR3b_extended'].isin(pep_cdr3s))\n",
    "            if sum(mask) == 0:\n",
    "                continue\n",
    "            # if we have cdr3s then grab them out\n",
    "            cdr3s = train_orig.loc[mask, 'CDR3b_extended']\n",
    "            # if there are more than we need randomly select\n",
    "            if len(cdr3s) > n_cdr3s:\n",
    "                irr_cdr3s += np.random.choice(cdr3s, size=n_cdr3s, replace=False).tolist()\n",
    "                break\n",
    "            # otherwise keep moving\n",
    "            else:\n",
    "                irr_cdr3s += cdr3s.tolist()\n",
    "                n_cdr3s -= len(cdr3s)\n",
    "        # compile the full list\n",
    "        irr = pd.DataFrame(irr_cdr3s, columns=['CDR3b_extended'])\n",
    "        irr['Peptide'] = pep\n",
    "        irrs.append(irr)\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "    ## CONVERT TO CORRECT FORMAT\n",
    "    # create X for training\n",
    "    X_train = pd.concat([pd.concat(irrs, axis=0), train], axis=0)\n",
    "    X_train_cdr3s = cdr3_to_X.loc[X_train['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_train_epitopes = ag_to_X.loc[X_train['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_train_cdr3s.columns = 'cdr3:' + X_train_cdr3s.columns\n",
    "    X_train_epitopes.columns = 'ag:' + X_train_epitopes.columns\n",
    "    X_train = X_train_cdr3s.join(X_train_epitopes)\n",
    "\n",
    "    # grab y for training\n",
    "    y_train = pd.Series(0, index=X_train.index)\n",
    "    y_train.iloc[-train.shape[0]:] = 1\n",
    "\n",
    "    # confirm the same length\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "    # create X for testing\n",
    "    X_test_cdr3s = cdr3_to_X.loc[test['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_test_epitopes = ag_to_X.loc[test['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_test_cdr3s.columns = 'cdr3:' + X_test_cdr3s.columns\n",
    "    X_test_epitopes.columns = 'ag:' + X_test_epitopes.columns\n",
    "    X_test = X_test_cdr3s.join(X_test_epitopes)\n",
    "\n",
    "    # remove constant columns\n",
    "    X_train = X_train.loc[:, X_train.nunique(0) > 1]\n",
    "    X_train = X_train.loc[:, X_train.sum(0) > 0]\n",
    "    # read in normalization factors\n",
    "    means = X_train.mean(0)\n",
    "    stds = (X_train - means).std(0)\n",
    "    # subset for relevant columns\n",
    "    X_train = X_train[means.index]\n",
    "    X_test = X_test[means.index]\n",
    "    # normalize\n",
    "    X_train -= means\n",
    "    X_train /= stds\n",
    "    X_test -= means\n",
    "    X_test /= stds\n",
    "\n",
    "\n",
    "    ## SETUP MODEL\n",
    "    # retrieve the appropriate columns\n",
    "    cols_cdr3 = X_train.columns[X_train.columns.str.startswith('cdr3')]\n",
    "    cols_ag = X_train.columns[X_train.columns.str.startswith('ag')]\n",
    "\n",
    "    # determine model parameters\n",
    "    # > layer for cdr3 alone\n",
    "    input_1 = keras.layers.Input(shape=(len(cols_cdr3)))\n",
    "    output_1 = keras.layers.Dense(300, activation='sigmoid')(input_1)\n",
    "    # > layer for ag alone\n",
    "    input_2 = keras.layers.Input(shape=(len(cols_ag)))\n",
    "    output_2 = keras.layers.Dense(200, activation='sigmoid')(input_2)\n",
    "    # > combined layer\n",
    "    concat_3 = keras.layers.Concatenate()([output_1, output_2])\n",
    "    output_3 = keras.layers.Dense(200, activation='sigmoid')(concat_3)\n",
    "    # > final logit softmax layer\n",
    "    output_4 = keras.layers.Dense(1, activation='sigmoid')(output_3)\n",
    "    model = keras.Model(inputs=[input_1, input_2], outputs=[output_4])\n",
    "    # set up the training parameters for the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC'],\n",
    "    )\n",
    "    # train the model\n",
    "    history = model.fit([X_train[cols_cdr3], X_train[cols_ag]], y_train,\n",
    "                        epochs=10,\n",
    "                        validation_data=([X_train[cols_cdr3], X_train[cols_ag]], y_train),\n",
    "                        workers=40, use_multiprocessing=True)\n",
    "\n",
    "    # testing predictions\n",
    "    y_pred = model.predict([X_test[cols_cdr3], X_test[cols_ag]],\n",
    "                           workers=40, use_multiprocessing=True)[:, 0]\n",
    "    y_pred = pd.Series(y_pred, index=test.index, name='Pred%')\n",
    "    hla2pred[hla] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee9e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back in predictions\n",
    "test = test_orig.copy()\n",
    "test['Prediction'] = np.nan\n",
    "for pred in hla2pred.values():\n",
    "    test.loc[pred.index, 'Prediction'] = pred\n",
    "# write down predictions\n",
    "test.to_csv('../external/tcr-specificity-prediction-challenge/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2efa34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fada190",
   "metadata": {},
   "source": [
    "## tcrb only v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb64f90d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve hla specific predictions\n",
    "hla2pred = {}\n",
    "for hla in test_orig['HLA'].unique():\n",
    "    # subset for HLA\n",
    "    train = train_orig.loc[train_orig['HLA'] == hla].copy()\n",
    "    test = test_orig.loc[test_orig['HLA'] == hla].copy()\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    # set seed and identify irrelevant matches\n",
    "    np.random.seed(0)\n",
    "    irrs = []\n",
    "\n",
    "    # re count the peptides for this HLA\n",
    "    counts = train['Peptide'].value_counts()\n",
    "    for pep in tqdm(counts.index):\n",
    "        # gather peptide cdr3 information\n",
    "        n_cdr3s = counts.loc[pep]\n",
    "        pep_cdr3s = train.loc[train['Peptide'] == pep, 'CDR3b_extended'].unique()\n",
    "        # we first grab the peptide levenshtein distances\n",
    "        pep_levenshtein = train_l[pep].sort_values()[::-1]\n",
    "        # systematically identify negative controls\n",
    "        irr_cdr3s = []\n",
    "        while n_cdr3s > 0:\n",
    "            # then we look at the max distance, gather those peptides, randomly choose one\n",
    "            # find associated CDR3s that don't overlap with the current peptide\n",
    "            vmax = pep_levenshtein.max()\n",
    "            irr_peps = pep_levenshtein.index[pep_levenshtein == vmax]\n",
    "            # reset for the next round if needed\n",
    "            pep_levenshtein = pep_levenshtein[pep_levenshtein < vmax]\n",
    "            # find the irrelevant peptide CDR3s and make sure they don't overlap\n",
    "            mask = (train_orig['Peptide'].isin(irr_peps)) & (~train_orig['CDR3b_extended'].isin(pep_cdr3s))\n",
    "            if sum(mask) == 0:\n",
    "                continue\n",
    "            # if we have cdr3s then grab them out\n",
    "            cdr3s = train_orig.loc[mask, 'CDR3b_extended']\n",
    "            # if there are more than we need randomly select\n",
    "            if len(cdr3s) > n_cdr3s:\n",
    "                irr_cdr3s += np.random.choice(cdr3s, size=n_cdr3s, replace=False).tolist()\n",
    "                break\n",
    "            # otherwise keep moving\n",
    "            else:\n",
    "                irr_cdr3s += cdr3s.tolist()\n",
    "                n_cdr3s -= len(cdr3s)\n",
    "        # compile the full list\n",
    "        irr = pd.DataFrame(irr_cdr3s, columns=['CDR3b_extended'])\n",
    "        irr['Peptide'] = pep\n",
    "        irrs.append(irr)\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "    ## CONVERT TO CORRECT FORMAT\n",
    "    # create X for training\n",
    "    X_train = pd.concat([pd.concat(irrs, axis=0), train], axis=0)\n",
    "    X_train_cdr3s = cdr3_to_X.loc[X_train['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_train_epitopes = ag_to_X.loc[X_train['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_train_cdr3s.columns = 'cdr3:' + X_train_cdr3s.columns\n",
    "    X_train_epitopes.columns = 'ag:' + X_train_epitopes.columns\n",
    "    X_train = X_train_cdr3s.join(X_train_epitopes)\n",
    "\n",
    "    # grab y for training\n",
    "    y_train = pd.Series(0, index=X_train.index)\n",
    "    y_train.iloc[-train.shape[0]:] = 1\n",
    "\n",
    "    # confirm the same length\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "    # create X for testing\n",
    "    X_test_cdr3s = cdr3_to_X.loc[test['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_test_epitopes = ag_to_X.loc[test['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_test_cdr3s.columns = 'cdr3:' + X_test_cdr3s.columns\n",
    "    X_test_epitopes.columns = 'ag:' + X_test_epitopes.columns\n",
    "    X_test = X_test_cdr3s.join(X_test_epitopes)\n",
    "\n",
    "    # remove constant columns\n",
    "    X_train = X_train.loc[:, X_train.nunique(0) > 1]\n",
    "    X_train = X_train.loc[:, X_train.sum(0) > 0]\n",
    "    # read in normalization factors\n",
    "    means = X_train.mean(0)\n",
    "    stds = (X_train - means).std(0)\n",
    "    # subset for relevant columns\n",
    "    X_train = X_train[means.index]\n",
    "    X_test = X_test[means.index]\n",
    "    # normalize\n",
    "    X_train -= means\n",
    "    X_train /= stds\n",
    "    X_test -= means\n",
    "    X_test /= stds\n",
    "\n",
    "\n",
    "    ## SETUP MODEL\n",
    "    # retrieve the appropriate columns\n",
    "    cols_cdr3 = X_train.columns[X_train.columns.str.startswith('cdr3')]\n",
    "    cols_ag = X_train.columns[X_train.columns.str.startswith('ag')]\n",
    "\n",
    "    # determine model parameters\n",
    "    # > layer for cdr3 alone\n",
    "    input_1 = keras.layers.Input(shape=(len(cols_cdr3)))\n",
    "    output_1 = keras.layers.Dense(400, activation='sigmoid')(input_1)\n",
    "    # > layer for ag alone\n",
    "    input_2 = keras.layers.Input(shape=(len(cols_ag)))\n",
    "    output_2 = keras.layers.Dense(300, activation='sigmoid')(input_2)\n",
    "    # > combined layer\n",
    "    concat_3 = keras.layers.Concatenate()([output_1, output_2])\n",
    "    output_3 = keras.layers.Dense(300, activation='sigmoid')(concat_3)\n",
    "    # > final logit softmax layer\n",
    "    output_4 = keras.layers.Dense(1, activation='sigmoid')(output_3)\n",
    "    model = keras.Model(inputs=[input_1, input_2], outputs=[output_4])\n",
    "    # set up the training parameters for the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC'],\n",
    "    )\n",
    "    # train the model\n",
    "    history = model.fit([X_train[cols_cdr3], X_train[cols_ag]], y_train,\n",
    "                        epochs=10,\n",
    "                        validation_data=([X_train[cols_cdr3], X_train[cols_ag]], y_train),\n",
    "                        workers=40, use_multiprocessing=True)\n",
    "\n",
    "    # testing predictions\n",
    "    y_pred = model.predict([X_test[cols_cdr3], X_test[cols_ag]],\n",
    "                           workers=40, use_multiprocessing=True)[:, 0]\n",
    "    y_pred = pd.Series(y_pred, index=test.index, name='Pred%')\n",
    "    hla2pred[hla] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back in predictions\n",
    "test = test_orig.copy()\n",
    "test['Prediction'] = np.nan\n",
    "for pred in hla2pred.values():\n",
    "    test.loc[pred.index, 'Prediction'] = pred\n",
    "# write down predictions\n",
    "test.to_csv('../external/tcr-specificity-prediction-challenge/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a587a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4be47568",
   "metadata": {},
   "source": [
    "## tcrb only v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd460caa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve hla specific predictions\n",
    "hla2pred = {}\n",
    "for hla in test_orig['HLA'].unique():\n",
    "    # subset for HLA\n",
    "    train = train_orig.loc[train_orig['HLA'] == hla].copy()\n",
    "    test = test_orig.loc[test_orig['HLA'] == hla].copy()\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    # set seed and identify irrelevant matches\n",
    "    np.random.seed(0)\n",
    "    irrs = []\n",
    "\n",
    "    # re count the peptides for this HLA\n",
    "    counts = train['Peptide'].value_counts()\n",
    "    for pep in tqdm(counts.index):\n",
    "        # gather peptide cdr3 information\n",
    "        n_cdr3s = counts.loc[pep]\n",
    "        pep_cdr3s = train.loc[train['Peptide'] == pep, 'CDR3b_extended'].unique()\n",
    "        # we first grab the peptide levenshtein distances\n",
    "        pep_levenshtein = train_l[pep].sort_values()[::-1]\n",
    "        # systematically identify negative controls\n",
    "        irr_cdr3s = []\n",
    "        while n_cdr3s > 0:\n",
    "            # then we look at the max distance, gather those peptides, randomly choose one\n",
    "            # find associated CDR3s that don't overlap with the current peptide\n",
    "            vmax = pep_levenshtein.max()\n",
    "            irr_peps = pep_levenshtein.index[pep_levenshtein == vmax]\n",
    "            # reset for the next round if needed\n",
    "            pep_levenshtein = pep_levenshtein[pep_levenshtein < vmax]\n",
    "            # find the irrelevant peptide CDR3s and make sure they don't overlap\n",
    "            mask = (train_orig['Peptide'].isin(irr_peps)) & (~train_orig['CDR3b_extended'].isin(pep_cdr3s))\n",
    "            if sum(mask) == 0:\n",
    "                continue\n",
    "            # if we have cdr3s then grab them out\n",
    "            cdr3s = train_orig.loc[mask, 'CDR3b_extended']\n",
    "            # if there are more than we need randomly select\n",
    "            if len(cdr3s) > n_cdr3s:\n",
    "                irr_cdr3s += np.random.choice(cdr3s, size=n_cdr3s, replace=False).tolist()\n",
    "                break\n",
    "            # otherwise keep moving\n",
    "            else:\n",
    "                irr_cdr3s += cdr3s.tolist()\n",
    "                n_cdr3s -= len(cdr3s)\n",
    "        # compile the full list\n",
    "        irr = pd.DataFrame(irr_cdr3s, columns=['CDR3b_extended'])\n",
    "        irr['Peptide'] = pep\n",
    "        irrs.append(irr)\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "    ## CONVERT TO CORRECT FORMAT\n",
    "    # create X for training\n",
    "    X_train = pd.concat([pd.concat(irrs, axis=0), train], axis=0)\n",
    "    X_train_cdr3s = cdr3_to_X.loc[X_train['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_train_epitopes = ag_to_X.loc[X_train['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_train_cdr3s.columns = 'cdr3:' + X_train_cdr3s.columns\n",
    "    X_train_epitopes.columns = 'ag:' + X_train_epitopes.columns\n",
    "    X_train = X_train_cdr3s.join(X_train_epitopes)\n",
    "\n",
    "    # grab y for training\n",
    "    y_train = pd.Series(0, index=X_train.index)\n",
    "    y_train.iloc[-train.shape[0]:] = 1\n",
    "\n",
    "    # confirm the same length\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "    # create X for testing\n",
    "    X_test_cdr3s = cdr3_to_X.loc[test['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_test_epitopes = ag_to_X.loc[test['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_test_cdr3s.columns = 'cdr3:' + X_test_cdr3s.columns\n",
    "    X_test_epitopes.columns = 'ag:' + X_test_epitopes.columns\n",
    "    X_test = X_test_cdr3s.join(X_test_epitopes)\n",
    "\n",
    "    # remove constant columns\n",
    "    X_train = X_train.loc[:, X_train.nunique(0) > 1]\n",
    "    X_train = X_train.loc[:, X_train.sum(0) > 0]\n",
    "    # read in normalization factors\n",
    "    means = X_train.mean(0)\n",
    "    stds = (X_train - means).std(0)\n",
    "    # subset for relevant columns\n",
    "    X_train = X_train[means.index]\n",
    "    X_test = X_test[means.index]\n",
    "    # normalize\n",
    "    X_train -= means\n",
    "    X_train /= stds\n",
    "    X_test -= means\n",
    "    X_test /= stds\n",
    "\n",
    "\n",
    "    ## SETUP MODEL\n",
    "    # retrieve the appropriate columns\n",
    "    cols_cdr3 = X_train.columns[X_train.columns.str.startswith('cdr3')]\n",
    "    cols_ag = X_train.columns[X_train.columns.str.startswith('ag')]\n",
    "\n",
    "    # determine model parameters\n",
    "    # > layer for cdr3 alone\n",
    "    input_1 = keras.layers.Input(shape=(len(cols_cdr3)))\n",
    "    output_1 = keras.layers.Dense(400, activation='sigmoid')(input_1)\n",
    "    # > layer for ag alone\n",
    "    input_2 = keras.layers.Input(shape=(len(cols_ag)))\n",
    "    output_2 = keras.layers.Dense(200, activation='sigmoid')(input_2)\n",
    "    # > combined layer\n",
    "    concat_3 = keras.layers.Concatenate()([output_1, output_2])\n",
    "    output_3 = keras.layers.Dense(200, activation='sigmoid')(concat_3)\n",
    "    # > final logit softmax layer\n",
    "    output_4 = keras.layers.Dense(1, activation='sigmoid')(output_3)\n",
    "    model = keras.Model(inputs=[input_1, input_2], outputs=[output_4])\n",
    "    # set up the training parameters for the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC'],\n",
    "    )\n",
    "    # train the model\n",
    "    history = model.fit([X_train[cols_cdr3], X_train[cols_ag]], y_train,\n",
    "                        epochs=10,\n",
    "                        validation_data=([X_train[cols_cdr3], X_train[cols_ag]], y_train),\n",
    "                        workers=40, use_multiprocessing=True)\n",
    "\n",
    "    # testing predictions\n",
    "    y_pred = model.predict([X_test[cols_cdr3], X_test[cols_ag]],\n",
    "                           workers=40, use_multiprocessing=True)[:, 0]\n",
    "    y_pred = pd.Series(y_pred, index=test.index, name='Pred%')\n",
    "    hla2pred[hla] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ba763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back in predictions\n",
    "test = test_orig.copy()\n",
    "test['Prediction'] = np.nan\n",
    "for pred in hla2pred.values():\n",
    "    test.loc[pred.index, 'Prediction'] = pred\n",
    "# write down predictions\n",
    "test.to_csv('../external/tcr-specificity-prediction-challenge/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf4f36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e71a47c3",
   "metadata": {},
   "source": [
    "## tcrb only v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8063a548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve hla specific predictions\n",
    "hla2pred = {}\n",
    "for hla in test_orig['HLA'].unique():\n",
    "    # subset for HLA\n",
    "    train = train_orig.loc[train_orig['HLA'] == hla].copy()\n",
    "    test = test_orig.loc[test_orig['HLA'] == hla].copy()\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    # set seed and identify irrelevant matches\n",
    "    np.random.seed(0)\n",
    "    irrs = []\n",
    "\n",
    "    # re count the peptides for this HLA\n",
    "    counts = train['Peptide'].value_counts()\n",
    "    for pep in tqdm(counts.index):\n",
    "        # gather peptide cdr3 information\n",
    "        n_cdr3s = counts.loc[pep]\n",
    "        pep_cdr3s = train.loc[train['Peptide'] == pep, 'CDR3b_extended'].unique()\n",
    "        # we first grab the peptide levenshtein distances\n",
    "        pep_levenshtein = train_l[pep].sort_values()[::-1]\n",
    "        # systematically identify negative controls\n",
    "        irr_cdr3s = []\n",
    "        while n_cdr3s > 0:\n",
    "            # then we look at the max distance, gather those peptides, randomly choose one\n",
    "            # find associated CDR3s that don't overlap with the current peptide\n",
    "            vmax = pep_levenshtein.max()\n",
    "            irr_peps = pep_levenshtein.index[pep_levenshtein == vmax]\n",
    "            # reset for the next round if needed\n",
    "            pep_levenshtein = pep_levenshtein[pep_levenshtein < vmax]\n",
    "            # find the irrelevant peptide CDR3s and make sure they don't overlap\n",
    "            mask = (train_orig['Peptide'].isin(irr_peps)) & (~train_orig['CDR3b_extended'].isin(pep_cdr3s))\n",
    "            if sum(mask) == 0:\n",
    "                continue\n",
    "            # if we have cdr3s then grab them out\n",
    "            cdr3s = train_orig.loc[mask, 'CDR3b_extended']\n",
    "            # if there are more than we need randomly select\n",
    "            if len(cdr3s) > n_cdr3s:\n",
    "                irr_cdr3s += np.random.choice(cdr3s, size=n_cdr3s, replace=False).tolist()\n",
    "                break\n",
    "            # otherwise keep moving\n",
    "            else:\n",
    "                irr_cdr3s += cdr3s.tolist()\n",
    "                n_cdr3s -= len(cdr3s)\n",
    "        # compile the full list\n",
    "        irr = pd.DataFrame(irr_cdr3s, columns=['CDR3b_extended'])\n",
    "        irr['Peptide'] = pep\n",
    "        irrs.append(irr)\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "    ## CONVERT TO CORRECT FORMAT\n",
    "    # create X for training\n",
    "    X_train = pd.concat([pd.concat(irrs, axis=0), train], axis=0)\n",
    "    X_train_cdr3s = cdr3_to_X.loc[X_train['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_train_epitopes = ag_to_X.loc[X_train['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_train_cdr3s.columns = 'cdr3:' + X_train_cdr3s.columns\n",
    "    X_train_epitopes.columns = 'ag:' + X_train_epitopes.columns\n",
    "    X_train = X_train_cdr3s.join(X_train_epitopes)\n",
    "\n",
    "    # grab y for training\n",
    "    y_train = pd.Series(0, index=X_train.index)\n",
    "    y_train.iloc[-train.shape[0]:] = 1\n",
    "\n",
    "    # confirm the same length\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "    # create X for testing\n",
    "    X_test_cdr3s = cdr3_to_X.loc[test['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_test_epitopes = ag_to_X.loc[test['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_test_cdr3s.columns = 'cdr3:' + X_test_cdr3s.columns\n",
    "    X_test_epitopes.columns = 'ag:' + X_test_epitopes.columns\n",
    "    X_test = X_test_cdr3s.join(X_test_epitopes)\n",
    "\n",
    "    # remove constant columns\n",
    "    X_train = X_train.loc[:, X_train.nunique(0) > 1]\n",
    "    X_train = X_train.loc[:, X_train.sum(0) > 0]\n",
    "    # read in normalization factors\n",
    "    means = X_train.mean(0)\n",
    "    stds = (X_train - means).std(0)\n",
    "    # subset for relevant columns\n",
    "    X_train = X_train[means.index]\n",
    "    X_test = X_test[means.index]\n",
    "    # normalize\n",
    "    X_train -= means\n",
    "    X_train /= stds\n",
    "    X_test -= means\n",
    "    X_test /= stds\n",
    "\n",
    "\n",
    "    ## SETUP MODEL\n",
    "    # retrieve the appropriate columns\n",
    "    cols_cdr3 = X_train.columns[X_train.columns.str.startswith('cdr3')]\n",
    "    cols_ag = X_train.columns[X_train.columns.str.startswith('ag')]\n",
    "\n",
    "    # determine model parameters\n",
    "    # > layer for cdr3 alone\n",
    "    input_1 = keras.layers.Input(shape=(len(cols_cdr3)))\n",
    "    output_1 = keras.layers.Dense(1000, activation='sigmoid')(input_1)\n",
    "    # > layer for ag alone\n",
    "    input_2 = keras.layers.Input(shape=(len(cols_ag)))\n",
    "    output_2 = keras.layers.Dense(500, activation='sigmoid')(input_2)\n",
    "    # > combined layer\n",
    "    concat_3 = keras.layers.Concatenate()([output_1, output_2])\n",
    "    output_3 = keras.layers.Dense(500, activation='sigmoid')(concat_3)\n",
    "    # > final logit softmax layer\n",
    "    output_4 = keras.layers.Dense(1, activation='sigmoid')(output_3)\n",
    "    model = keras.Model(inputs=[input_1, input_2], outputs=[output_4])\n",
    "    # set up the training parameters for the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC'],\n",
    "    )\n",
    "    # train the model\n",
    "    history = model.fit([X_train[cols_cdr3], X_train[cols_ag]], y_train,\n",
    "                        epochs=10,\n",
    "                        validation_data=([X_train[cols_cdr3], X_train[cols_ag]], y_train),\n",
    "                        workers=40, use_multiprocessing=True)\n",
    "\n",
    "    # testing predictions\n",
    "    y_pred = model.predict([X_test[cols_cdr3], X_test[cols_ag]],\n",
    "                           workers=40, use_multiprocessing=True)[:, 0]\n",
    "    y_pred = pd.Series(y_pred, index=test.index, name='Pred%')\n",
    "    hla2pred[hla] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684e072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back in predictions\n",
    "test = test_orig.copy()\n",
    "test['Prediction'] = np.nan\n",
    "for pred in hla2pred.values():\n",
    "    test.loc[pred.index, 'Prediction'] = pred\n",
    "# write down predictions\n",
    "test.to_csv('../external/tcr-specificity-prediction-challenge/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6118ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53b6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb4c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59605497",
   "metadata": {},
   "source": [
    "## tcra and tcrb only v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve encodings\n",
    "cdr3a_to_X = encode_cdr3s(pd.concat([train_orig, test_orig], axis=0)['CDR3a_extended'].dropna().unique(), 40, False)\n",
    "cdr3b_to_X = encode_cdr3s(pd.concat([train_orig, test_orig], axis=0)['CDR3b_extended'].dropna().unique(), 40, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e8431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve hla specific predictions\n",
    "hla2pred = {}\n",
    "for hla in test_orig['HLA'].unique():\n",
    "    # subset for HLA\n",
    "    train = train_orig.loc[train_orig['HLA'] == hla].copy()\n",
    "    test = test_orig.loc[test_orig['HLA'] == hla].copy()\n",
    "    train = train.dropna(subset=['CDR3a_extended'])\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    # set seed and identify irrelevant matches\n",
    "    np.random.seed(0)\n",
    "    irrs = []\n",
    "\n",
    "    # re count the peptides for this HLA\n",
    "    counts = train['Peptide'].value_counts()\n",
    "    for pep in tqdm(counts.index):\n",
    "        # gather peptide cdr3 information\n",
    "        n_cdr3s = counts.loc[pep]\n",
    "        pep_cdr3s = train.loc[train['Peptide'] == pep, ['CDR3a_extended','CDR3b_extended']].value_counts().reset_index()\n",
    "        # we first grab the peptide levenshtein distances\n",
    "        pep_levenshtein = train_l[pep].sort_values()[::-1]\n",
    "        # systematically identify negative controls\n",
    "        irr_cdr3s = []\n",
    "        while n_cdr3s > 0:\n",
    "            # then we look at the max distance, gather those peptides, randomly choose one\n",
    "            # find associated CDR3s that don't overlap with the current peptide\n",
    "            vmax = pep_levenshtein.max()\n",
    "            irr_peps = pep_levenshtein.index[pep_levenshtein == vmax]\n",
    "            # reset for the next round if needed\n",
    "            pep_levenshtein = pep_levenshtein[pep_levenshtein < vmax]\n",
    "            # find the irrelevant peptide CDR3s and make sure they don't overlap\n",
    "            mask = (train_orig['Peptide'].isin(irr_peps)) &\\\n",
    "            ((~train_orig['CDR3a_extended'].isin(pep_cdr3s['CDR3a_extended'])) |\\\n",
    "            (~train_orig['CDR3b_extended'].isin(pep_cdr3s['CDR3b_extended']))) &\\\n",
    "            (~train_orig['CDR3a_extended'].isna()) &\\\n",
    "            (~train_orig['CDR3b_extended'].isna())\n",
    "            if sum(mask) == 0:\n",
    "                continue\n",
    "            # if we have cdr3s then grab them out\n",
    "            cdr3s = train_orig.loc[mask, ['CDR3a_extended','CDR3b_extended']]\n",
    "            # if there are more than we need randomly select\n",
    "            if len(cdr3s) > n_cdr3s:\n",
    "                irr_cdr3s_idxs = np.random.choice(cdr3s.index, size=n_cdr3s, replace=False)\n",
    "                irr_cdr3s += cdr3s.loc[irr_cdr3s_idxs].values.tolist()\n",
    "                break\n",
    "            # otherwise keep moving\n",
    "            else:\n",
    "                irr_cdr3s += cdr3s.values.tolist()\n",
    "                n_cdr3s -= len(cdr3s)\n",
    "        # compile the full list\n",
    "        irr = pd.DataFrame(irr_cdr3s, columns=['CDR3a_extended','CDR3b_extended'])\n",
    "        irr['Peptide'] = pep\n",
    "        irrs.append(irr)\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "    ## CONVERT TO CORRECT FORMAT\n",
    "    # create X for training\n",
    "    X_train = pd.concat([pd.concat(irrs, axis=0), train], axis=0)\n",
    "    X_train_cdr3as = cdr3a_to_X.loc[X_train['CDR3a_extended']].reset_index().iloc[:, 1:]\n",
    "    X_train_cdr3bs = cdr3b_to_X.loc[X_train['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_train_epitopes = ag_to_X.loc[X_train['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_train_cdr3as.columns = 'cdr3a:' + X_train_cdr3as.columns\n",
    "    X_train_cdr3bs.columns = 'cdr3b:' + X_train_cdr3bs.columns\n",
    "    X_train_epitopes.columns = 'ag:' + X_train_epitopes.columns\n",
    "    X_train = X_train_cdr3as.join(X_train_cdr3bs).join(X_train_epitopes)\n",
    "\n",
    "    # grab y for training\n",
    "    y_train = pd.Series(0, index=X_train.index)\n",
    "    y_train.iloc[-train.shape[0]:] = 1\n",
    "\n",
    "    # confirm the same length\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "    # create X for testing\n",
    "    X_test_cdr3as = cdr3a_to_X.loc[test['CDR3a_extended']].reset_index().iloc[:, 1:]\n",
    "    X_test_cdr3bs = cdr3b_to_X.loc[test['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_test_epitopes = ag_to_X.loc[test['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_test_cdr3as.columns = 'cdr3a:' + X_test_cdr3as.columns\n",
    "    X_test_cdr3bs.columns = 'cdr3b:' + X_test_cdr3bs.columns\n",
    "    X_test_epitopes.columns = 'ag:' + X_test_epitopes.columns\n",
    "    X_test = X_test_cdr3as.join(X_test_cdr3bs).join(X_test_epitopes)\n",
    "\n",
    "    # remove constant columns\n",
    "    X_train = X_train.loc[:, X_train.nunique(0) > 1]\n",
    "    X_train = X_train.loc[:, X_train.sum(0) > 0]\n",
    "    # read in normalization factors\n",
    "    means = X_train.mean(0)\n",
    "    stds = (X_train - means).std(0)\n",
    "    # subset for relevant columns\n",
    "    X_train = X_train[means.index]\n",
    "    X_test = X_test[means.index]\n",
    "    # normalize\n",
    "    X_train -= means\n",
    "    X_train /= stds\n",
    "    X_test -= means\n",
    "    X_test /= stds\n",
    "\n",
    "\n",
    "    ## SETUP MODEL\n",
    "    # retrieve the appropriate columns\n",
    "    cols_cdr3a = X_train.columns[X_train.columns.str.startswith('cdr3a')]\n",
    "    cols_cdr3b = X_train.columns[X_train.columns.str.startswith('cdr3b')]\n",
    "    cols_ag = X_train.columns[X_train.columns.str.startswith('ag')]\n",
    "\n",
    "    # determine model parameters\n",
    "    # > layer for cdr3a alone\n",
    "    input_0 = keras.layers.Input(shape=(len(cols_cdr3a)))\n",
    "    output_0 = keras.layers.Dense(200, activation='sigmoid')(input_0)\n",
    "    # > layer for cdr3b alone\n",
    "    input_1 = keras.layers.Input(shape=(len(cols_cdr3b)))\n",
    "    output_1 = keras.layers.Dense(200, activation='sigmoid')(input_1)\n",
    "    # > layer for ag alone\n",
    "    input_2 = keras.layers.Input(shape=(len(cols_ag)))\n",
    "    output_2 = keras.layers.Dense(100, activation='sigmoid')(input_2)\n",
    "    # > combined layer for tcra and tcrb\n",
    "    concat_3 = keras.layers.Concatenate()([output_0, output_1])\n",
    "    output_3 = keras.layers.Dense(100, activation='sigmoid')(concat_3)\n",
    "    # > combined layer for tcr and ag\n",
    "    concat_4 = keras.layers.Concatenate()([output_2, output_3])\n",
    "    output_4 = keras.layers.Dense(100, activation='sigmoid')(concat_4)\n",
    "    # > final logit softmax layer\n",
    "    output_5 = keras.layers.Dense(1, activation='sigmoid')(output_4)\n",
    "    model = keras.Model(inputs=[input_0, input_1, input_2], outputs=[output_5])\n",
    "    # set up the training parameters for the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC'],\n",
    "    )\n",
    "    # train the model\n",
    "    history = model.fit([X_train[cols_cdr3a], X_train[cols_cdr3b], X_train[cols_ag]], y_train,\n",
    "                        epochs=10,\n",
    "                        validation_data=([X_train[cols_cdr3a], X_train[cols_cdr3b], X_train[cols_ag]], y_train),\n",
    "                        workers=40, use_multiprocessing=True)\n",
    "\n",
    "    # testing predictions\n",
    "    y_pred = model.predict([X_test[cols_cdr3a], X_test[cols_cdr3b], X_test[cols_ag]],\n",
    "                           workers=40, use_multiprocessing=True)[:, 0]\n",
    "    y_pred = pd.Series(y_pred, index=test.index, name='Pred%')\n",
    "\n",
    "    hla2pred[hla] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back in predictions\n",
    "test = test_orig.copy()\n",
    "test['Prediction'] = np.nan\n",
    "for pred in hla2pred.values():\n",
    "    test.loc[pred.index, 'Prediction'] = pred\n",
    "# write down predictions\n",
    "test.to_csv('../external/tcr-specificity-prediction-challenge/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b615c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27d04bd9",
   "metadata": {},
   "source": [
    "## tcra and tcrb only v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f260d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve hla specific predictions\n",
    "hla2pred = {}\n",
    "for hla in test_orig['HLA'].unique():\n",
    "    # subset for HLA\n",
    "    train = train_orig.loc[train_orig['HLA'] == hla].copy()\n",
    "    test = test_orig.loc[test_orig['HLA'] == hla].copy()\n",
    "    train = train.dropna(subset=['CDR3a_extended'])\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    # set seed and identify irrelevant matches\n",
    "    np.random.seed(0)\n",
    "    irrs = []\n",
    "\n",
    "    # re count the peptides for this HLA\n",
    "    counts = train['Peptide'].value_counts()\n",
    "    for pep in tqdm(counts.index):\n",
    "        # gather peptide cdr3 information\n",
    "        n_cdr3s = counts.loc[pep]\n",
    "        pep_cdr3s = train.loc[train['Peptide'] == pep, ['CDR3a_extended','CDR3b_extended']].value_counts().reset_index()\n",
    "        # we first grab the peptide levenshtein distances\n",
    "        pep_levenshtein = train_l[pep].sort_values()[::-1]\n",
    "        # systematically identify negative controls\n",
    "        irr_cdr3s = []\n",
    "        while n_cdr3s > 0:\n",
    "            # then we look at the max distance, gather those peptides, randomly choose one\n",
    "            # find associated CDR3s that don't overlap with the current peptide\n",
    "            vmax = pep_levenshtein.max()\n",
    "            irr_peps = pep_levenshtein.index[pep_levenshtein == vmax]\n",
    "            # reset for the next round if needed\n",
    "            pep_levenshtein = pep_levenshtein[pep_levenshtein < vmax]\n",
    "            # find the irrelevant peptide CDR3s and make sure they don't overlap\n",
    "            mask = (train_orig['Peptide'].isin(irr_peps)) &\\\n",
    "            ((~train_orig['CDR3a_extended'].isin(pep_cdr3s['CDR3a_extended'])) |\\\n",
    "            (~train_orig['CDR3b_extended'].isin(pep_cdr3s['CDR3b_extended']))) &\\\n",
    "            (~train_orig['CDR3a_extended'].isna()) &\\\n",
    "            (~train_orig['CDR3b_extended'].isna())\n",
    "            if sum(mask) == 0:\n",
    "                continue\n",
    "            # if we have cdr3s then grab them out\n",
    "            cdr3s = train_orig.loc[mask, ['CDR3a_extended','CDR3b_extended']]\n",
    "            # if there are more than we need randomly select\n",
    "            if len(cdr3s) > n_cdr3s:\n",
    "                irr_cdr3s_idxs = np.random.choice(cdr3s.index, size=n_cdr3s, replace=False)\n",
    "                irr_cdr3s += cdr3s.loc[irr_cdr3s_idxs].values.tolist()\n",
    "                break\n",
    "            # otherwise keep moving\n",
    "            else:\n",
    "                irr_cdr3s += cdr3s.values.tolist()\n",
    "                n_cdr3s -= len(cdr3s)\n",
    "        # compile the full list\n",
    "        irr = pd.DataFrame(irr_cdr3s, columns=['CDR3a_extended','CDR3b_extended'])\n",
    "        irr['Peptide'] = pep\n",
    "        irrs.append(irr)\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "    ## CONVERT TO CORRECT FORMAT\n",
    "    # create X for training\n",
    "    X_train = pd.concat([pd.concat(irrs, axis=0), train], axis=0)\n",
    "    X_train_cdr3as = cdr3a_to_X.loc[X_train['CDR3a_extended']].reset_index().iloc[:, 1:]\n",
    "    X_train_cdr3bs = cdr3b_to_X.loc[X_train['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_train_epitopes = ag_to_X.loc[X_train['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_train_cdr3as.columns = 'cdr3a:' + X_train_cdr3as.columns\n",
    "    X_train_cdr3bs.columns = 'cdr3b:' + X_train_cdr3bs.columns\n",
    "    X_train_epitopes.columns = 'ag:' + X_train_epitopes.columns\n",
    "    X_train = X_train_cdr3as.join(X_train_cdr3bs).join(X_train_epitopes)\n",
    "\n",
    "    # grab y for training\n",
    "    y_train = pd.Series(0, index=X_train.index)\n",
    "    y_train.iloc[-train.shape[0]:] = 1\n",
    "\n",
    "    # confirm the same length\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "    # create X for testing\n",
    "    X_test_cdr3as = cdr3a_to_X.loc[test['CDR3a_extended']].reset_index().iloc[:, 1:]\n",
    "    X_test_cdr3bs = cdr3b_to_X.loc[test['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_test_epitopes = ag_to_X.loc[test['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_test_cdr3as.columns = 'cdr3a:' + X_test_cdr3as.columns\n",
    "    X_test_cdr3bs.columns = 'cdr3b:' + X_test_cdr3bs.columns\n",
    "    X_test_epitopes.columns = 'ag:' + X_test_epitopes.columns\n",
    "    X_test = X_test_cdr3as.join(X_test_cdr3bs).join(X_test_epitopes)\n",
    "\n",
    "    # remove constant columns\n",
    "    X_train = X_train.loc[:, X_train.nunique(0) > 1]\n",
    "    X_train = X_train.loc[:, X_train.sum(0) > 0]\n",
    "    # read in normalization factors\n",
    "    means = X_train.mean(0)\n",
    "    stds = (X_train - means).std(0)\n",
    "    # subset for relevant columns\n",
    "    X_train = X_train[means.index]\n",
    "    X_test = X_test[means.index]\n",
    "    # normalize\n",
    "    X_train -= means\n",
    "    X_train /= stds\n",
    "    X_test -= means\n",
    "    X_test /= stds\n",
    "\n",
    "\n",
    "    ## SETUP MODEL\n",
    "    # retrieve the appropriate columns\n",
    "    cols_cdr3a = X_train.columns[X_train.columns.str.startswith('cdr3a')]\n",
    "    cols_cdr3b = X_train.columns[X_train.columns.str.startswith('cdr3b')]\n",
    "    cols_ag = X_train.columns[X_train.columns.str.startswith('ag')]\n",
    "\n",
    "    # determine model parameters\n",
    "    # > layer for cdr3a alone\n",
    "    input_0 = keras.layers.Input(shape=(len(cols_cdr3a)))\n",
    "    output_0 = keras.layers.Dense(300, activation='sigmoid')(input_0)\n",
    "    # > layer for cdr3b alone\n",
    "    input_1 = keras.layers.Input(shape=(len(cols_cdr3b)))\n",
    "    output_1 = keras.layers.Dense(300, activation='sigmoid')(input_1)\n",
    "    # > layer for ag alone\n",
    "    input_2 = keras.layers.Input(shape=(len(cols_ag)))\n",
    "    output_2 = keras.layers.Dense(200, activation='sigmoid')(input_2)\n",
    "    # > combined layer for tcra and tcrb\n",
    "    concat_3 = keras.layers.Concatenate()([output_0, output_1])\n",
    "    output_3 = keras.layers.Dense(100, activation='sigmoid')(concat_3)\n",
    "    # > combined layer for tcr and ag\n",
    "    concat_4 = keras.layers.Concatenate()([output_2, output_3])\n",
    "    output_4 = keras.layers.Dense(100, activation='sigmoid')(concat_4)\n",
    "    # > final logit softmax layer\n",
    "    output_5 = keras.layers.Dense(1, activation='sigmoid')(output_4)\n",
    "    model = keras.Model(inputs=[input_0, input_1, input_2], outputs=[output_5])\n",
    "    # set up the training parameters for the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC'],\n",
    "    )\n",
    "    # train the model\n",
    "    history = model.fit([X_train[cols_cdr3a], X_train[cols_cdr3b], X_train[cols_ag]], y_train,\n",
    "                        epochs=10,\n",
    "                        validation_data=([X_train[cols_cdr3a], X_train[cols_cdr3b], X_train[cols_ag]], y_train),\n",
    "                        workers=40, use_multiprocessing=True)\n",
    "\n",
    "    # testing predictions\n",
    "    y_pred = model.predict([X_test[cols_cdr3a], X_test[cols_cdr3b], X_test[cols_ag]],\n",
    "                           workers=40, use_multiprocessing=True)[:, 0]\n",
    "    y_pred = pd.Series(y_pred, index=test.index, name='Pred%')\n",
    "\n",
    "    hla2pred[hla] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da1180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back in predictions\n",
    "test = test_orig.copy()\n",
    "test['Prediction'] = np.nan\n",
    "for pred in hla2pred.values():\n",
    "    test.loc[pred.index, 'Prediction'] = pred\n",
    "# write down predictions\n",
    "test.to_csv('../external/tcr-specificity-prediction-challenge/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1ea8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09105fa9",
   "metadata": {},
   "source": [
    "## tcra and tcrb only v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d73756d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve hla specific predictions\n",
    "hla2pred = {}\n",
    "for hla in test_orig['HLA'].unique():\n",
    "    # subset for HLA\n",
    "    train = train_orig.loc[train_orig['HLA'] == hla].copy()\n",
    "    test = test_orig.loc[test_orig['HLA'] == hla].copy()\n",
    "    train = train.dropna(subset=['CDR3a_extended'])\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    # set seed and identify irrelevant matches\n",
    "    np.random.seed(0)\n",
    "    irrs = []\n",
    "\n",
    "    # re count the peptides for this HLA\n",
    "    counts = train['Peptide'].value_counts()\n",
    "    for pep in tqdm(counts.index):\n",
    "        # gather peptide cdr3 information\n",
    "        n_cdr3s = counts.loc[pep]\n",
    "        pep_cdr3s = train.loc[train['Peptide'] == pep, ['CDR3a_extended','CDR3b_extended']].value_counts().reset_index()\n",
    "        # we first grab the peptide levenshtein distances\n",
    "        pep_levenshtein = train_l[pep].sort_values()[::-1]\n",
    "        # systematically identify negative controls\n",
    "        irr_cdr3s = []\n",
    "        while n_cdr3s > 0:\n",
    "            # then we look at the max distance, gather those peptides, randomly choose one\n",
    "            # find associated CDR3s that don't overlap with the current peptide\n",
    "            vmax = pep_levenshtein.max()\n",
    "            irr_peps = pep_levenshtein.index[pep_levenshtein == vmax]\n",
    "            # reset for the next round if needed\n",
    "            pep_levenshtein = pep_levenshtein[pep_levenshtein < vmax]\n",
    "            # find the irrelevant peptide CDR3s and make sure they don't overlap\n",
    "            mask = (train_orig['Peptide'].isin(irr_peps)) &\\\n",
    "            ((~train_orig['CDR3a_extended'].isin(pep_cdr3s['CDR3a_extended'])) |\\\n",
    "            (~train_orig['CDR3b_extended'].isin(pep_cdr3s['CDR3b_extended']))) &\\\n",
    "            (~train_orig['CDR3a_extended'].isna()) &\\\n",
    "            (~train_orig['CDR3b_extended'].isna())\n",
    "            if sum(mask) == 0:\n",
    "                continue\n",
    "            # if we have cdr3s then grab them out\n",
    "            cdr3s = train_orig.loc[mask, ['CDR3a_extended','CDR3b_extended']]\n",
    "            # if there are more than we need randomly select\n",
    "            if len(cdr3s) > n_cdr3s:\n",
    "                irr_cdr3s_idxs = np.random.choice(cdr3s.index, size=n_cdr3s, replace=False)\n",
    "                irr_cdr3s += cdr3s.loc[irr_cdr3s_idxs].values.tolist()\n",
    "                break\n",
    "            # otherwise keep moving\n",
    "            else:\n",
    "                irr_cdr3s += cdr3s.values.tolist()\n",
    "                n_cdr3s -= len(cdr3s)\n",
    "        # compile the full list\n",
    "        irr = pd.DataFrame(irr_cdr3s, columns=['CDR3a_extended','CDR3b_extended'])\n",
    "        irr['Peptide'] = pep\n",
    "        irrs.append(irr)\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "    ## CONVERT TO CORRECT FORMAT\n",
    "    # create X for training\n",
    "    X_train = pd.concat([pd.concat(irrs, axis=0), train], axis=0)\n",
    "    X_train_cdr3as = cdr3a_to_X.loc[X_train['CDR3a_extended']].reset_index().iloc[:, 1:]\n",
    "    X_train_cdr3bs = cdr3b_to_X.loc[X_train['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_train_epitopes = ag_to_X.loc[X_train['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_train_cdr3as.columns = 'cdr3a:' + X_train_cdr3as.columns\n",
    "    X_train_cdr3bs.columns = 'cdr3b:' + X_train_cdr3bs.columns\n",
    "    X_train_epitopes.columns = 'ag:' + X_train_epitopes.columns\n",
    "    X_train = X_train_cdr3as.join(X_train_cdr3bs).join(X_train_epitopes)\n",
    "\n",
    "    # grab y for training\n",
    "    y_train = pd.Series(0, index=X_train.index)\n",
    "    y_train.iloc[-train.shape[0]:] = 1\n",
    "\n",
    "    # confirm the same length\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "    # create X for testing\n",
    "    X_test_cdr3as = cdr3a_to_X.loc[test['CDR3a_extended']].reset_index().iloc[:, 1:]\n",
    "    X_test_cdr3bs = cdr3b_to_X.loc[test['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_test_epitopes = ag_to_X.loc[test['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_test_cdr3as.columns = 'cdr3a:' + X_test_cdr3as.columns\n",
    "    X_test_cdr3bs.columns = 'cdr3b:' + X_test_cdr3bs.columns\n",
    "    X_test_epitopes.columns = 'ag:' + X_test_epitopes.columns\n",
    "    X_test = X_test_cdr3as.join(X_test_cdr3bs).join(X_test_epitopes)\n",
    "\n",
    "    # remove constant columns\n",
    "    X_train = X_train.loc[:, X_train.nunique(0) > 1]\n",
    "    X_train = X_train.loc[:, X_train.sum(0) > 0]\n",
    "    # read in normalization factors\n",
    "    means = X_train.mean(0)\n",
    "    stds = (X_train - means).std(0)\n",
    "    # subset for relevant columns\n",
    "    X_train = X_train[means.index]\n",
    "    X_test = X_test[means.index]\n",
    "    # normalize\n",
    "    X_train -= means\n",
    "    X_train /= stds\n",
    "    X_test -= means\n",
    "    X_test /= stds\n",
    "\n",
    "\n",
    "    ## SETUP MODEL\n",
    "    # retrieve the appropriate columns\n",
    "    cols_cdr3a = X_train.columns[X_train.columns.str.startswith('cdr3a')]\n",
    "    cols_cdr3b = X_train.columns[X_train.columns.str.startswith('cdr3b')]\n",
    "    cols_ag = X_train.columns[X_train.columns.str.startswith('ag')]\n",
    "\n",
    "    # determine model parameters\n",
    "    # > layer for cdr3a alone\n",
    "    input_0 = keras.layers.Input(shape=(len(cols_cdr3a)))\n",
    "    output_0 = keras.layers.Dense(300, activation='sigmoid')(input_0)\n",
    "    # > layer for cdr3b alone\n",
    "    input_1 = keras.layers.Input(shape=(len(cols_cdr3b)))\n",
    "    output_1 = keras.layers.Dense(300, activation='sigmoid')(input_1)\n",
    "    # > layer for ag alone\n",
    "    input_2 = keras.layers.Input(shape=(len(cols_ag)))\n",
    "    output_2 = keras.layers.Dense(100, activation='sigmoid')(input_2)\n",
    "    # > combined layer for tcra and tcrb\n",
    "    concat_3 = keras.layers.Concatenate()([output_0, output_1])\n",
    "    output_3 = keras.layers.Dense(200, activation='sigmoid')(concat_3)\n",
    "    # > combined layer for tcr and ag\n",
    "    concat_4 = keras.layers.Concatenate()([output_2, output_3])\n",
    "    output_4 = keras.layers.Dense(100, activation='sigmoid')(concat_4)\n",
    "    # > final logit softmax layer\n",
    "    output_5 = keras.layers.Dense(1, activation='sigmoid')(output_4)\n",
    "    model = keras.Model(inputs=[input_0, input_1, input_2], outputs=[output_5])\n",
    "    # set up the training parameters for the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC'],\n",
    "    )\n",
    "    # train the model\n",
    "    history = model.fit([X_train[cols_cdr3a], X_train[cols_cdr3b], X_train[cols_ag]], y_train,\n",
    "                        epochs=10,\n",
    "                        validation_data=([X_train[cols_cdr3a], X_train[cols_cdr3b], X_train[cols_ag]], y_train),\n",
    "                        workers=40, use_multiprocessing=True)\n",
    "\n",
    "    # testing predictions\n",
    "    y_pred = model.predict([X_test[cols_cdr3a], X_test[cols_cdr3b], X_test[cols_ag]],\n",
    "                           workers=40, use_multiprocessing=True)[:, 0]\n",
    "    y_pred = pd.Series(y_pred, index=test.index, name='Pred%')\n",
    "\n",
    "    hla2pred[hla] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a38b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back in predictions\n",
    "test = test_orig.copy()\n",
    "test['Prediction'] = np.nan\n",
    "for pred in hla2pred.values():\n",
    "    test.loc[pred.index, 'Prediction'] = pred\n",
    "# write down predictions\n",
    "test.to_csv('../external/tcr-specificity-prediction-challenge/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd15203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1595a6cf",
   "metadata": {},
   "source": [
    "## tcra and tcrb only v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a39284e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve hla specific predictions\n",
    "hla2pred = {}\n",
    "for hla in test_orig['HLA'].unique():\n",
    "    # subset for HLA\n",
    "    train = train_orig.loc[train_orig['HLA'] == hla].copy()\n",
    "    test = test_orig.loc[test_orig['HLA'] == hla].copy()\n",
    "    train = train.dropna(subset=['CDR3a_extended'])\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    # set seed and identify irrelevant matches\n",
    "    np.random.seed(0)\n",
    "    irrs = []\n",
    "\n",
    "    # re count the peptides for this HLA\n",
    "    counts = train['Peptide'].value_counts()\n",
    "    for pep in tqdm(counts.index):\n",
    "        # gather peptide cdr3 information\n",
    "        n_cdr3s = counts.loc[pep]\n",
    "        pep_cdr3s = train.loc[train['Peptide'] == pep, ['CDR3a_extended','CDR3b_extended']].value_counts().reset_index()\n",
    "        # we first grab the peptide levenshtein distances\n",
    "        pep_levenshtein = train_l[pep].sort_values()[::-1]\n",
    "        # systematically identify negative controls\n",
    "        irr_cdr3s = []\n",
    "        while n_cdr3s > 0:\n",
    "            # then we look at the max distance, gather those peptides, randomly choose one\n",
    "            # find associated CDR3s that don't overlap with the current peptide\n",
    "            vmax = pep_levenshtein.max()\n",
    "            irr_peps = pep_levenshtein.index[pep_levenshtein == vmax]\n",
    "            # reset for the next round if needed\n",
    "            pep_levenshtein = pep_levenshtein[pep_levenshtein < vmax]\n",
    "            # find the irrelevant peptide CDR3s and make sure they don't overlap\n",
    "            mask = (train_orig['Peptide'].isin(irr_peps)) &\\\n",
    "            ((~train_orig['CDR3a_extended'].isin(pep_cdr3s['CDR3a_extended'])) |\\\n",
    "            (~train_orig['CDR3b_extended'].isin(pep_cdr3s['CDR3b_extended']))) &\\\n",
    "            (~train_orig['CDR3a_extended'].isna()) &\\\n",
    "            (~train_orig['CDR3b_extended'].isna())\n",
    "            if sum(mask) == 0:\n",
    "                continue\n",
    "            # if we have cdr3s then grab them out\n",
    "            cdr3s = train_orig.loc[mask, ['CDR3a_extended','CDR3b_extended']]\n",
    "            # if there are more than we need randomly select\n",
    "            if len(cdr3s) > n_cdr3s:\n",
    "                irr_cdr3s_idxs = np.random.choice(cdr3s.index, size=n_cdr3s, replace=False)\n",
    "                irr_cdr3s += cdr3s.loc[irr_cdr3s_idxs].values.tolist()\n",
    "                break\n",
    "            # otherwise keep moving\n",
    "            else:\n",
    "                irr_cdr3s += cdr3s.values.tolist()\n",
    "                n_cdr3s -= len(cdr3s)\n",
    "        # compile the full list\n",
    "        irr = pd.DataFrame(irr_cdr3s, columns=['CDR3a_extended','CDR3b_extended'])\n",
    "        irr['Peptide'] = pep\n",
    "        irrs.append(irr)\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "    ## CONVERT TO CORRECT FORMAT\n",
    "    # create X for training\n",
    "    X_train = pd.concat([pd.concat(irrs, axis=0), train], axis=0)\n",
    "    X_train_cdr3as = cdr3a_to_X.loc[X_train['CDR3a_extended']].reset_index().iloc[:, 1:]\n",
    "    X_train_cdr3bs = cdr3b_to_X.loc[X_train['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_train_epitopes = ag_to_X.loc[X_train['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_train_cdr3as.columns = 'cdr3a:' + X_train_cdr3as.columns\n",
    "    X_train_cdr3bs.columns = 'cdr3b:' + X_train_cdr3bs.columns\n",
    "    X_train_epitopes.columns = 'ag:' + X_train_epitopes.columns\n",
    "    X_train = X_train_cdr3as.join(X_train_cdr3bs).join(X_train_epitopes)\n",
    "\n",
    "    # grab y for training\n",
    "    y_train = pd.Series(0, index=X_train.index)\n",
    "    y_train.iloc[-train.shape[0]:] = 1\n",
    "\n",
    "    # confirm the same length\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "    # create X for testing\n",
    "    X_test_cdr3as = cdr3a_to_X.loc[test['CDR3a_extended']].reset_index().iloc[:, 1:]\n",
    "    X_test_cdr3bs = cdr3b_to_X.loc[test['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_test_epitopes = ag_to_X.loc[test['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_test_cdr3as.columns = 'cdr3a:' + X_test_cdr3as.columns\n",
    "    X_test_cdr3bs.columns = 'cdr3b:' + X_test_cdr3bs.columns\n",
    "    X_test_epitopes.columns = 'ag:' + X_test_epitopes.columns\n",
    "    X_test = X_test_cdr3as.join(X_test_cdr3bs).join(X_test_epitopes)\n",
    "\n",
    "    # remove constant columns\n",
    "    X_train = X_train.loc[:, X_train.nunique(0) > 1]\n",
    "    X_train = X_train.loc[:, X_train.sum(0) > 0]\n",
    "    # read in normalization factors\n",
    "    means = X_train.mean(0)\n",
    "    stds = (X_train - means).std(0)\n",
    "    # subset for relevant columns\n",
    "    X_train = X_train[means.index]\n",
    "    X_test = X_test[means.index]\n",
    "    # normalize\n",
    "    X_train -= means\n",
    "    X_train /= stds\n",
    "    X_test -= means\n",
    "    X_test /= stds\n",
    "\n",
    "\n",
    "    ## SETUP MODEL\n",
    "    # retrieve the appropriate columns\n",
    "    cols_cdr3a = X_train.columns[X_train.columns.str.startswith('cdr3a')]\n",
    "    cols_cdr3b = X_train.columns[X_train.columns.str.startswith('cdr3b')]\n",
    "    cols_ag = X_train.columns[X_train.columns.str.startswith('ag')]\n",
    "\n",
    "    # determine model parameters\n",
    "    # > layer for cdr3a alone\n",
    "    input_0 = keras.layers.Input(shape=(len(cols_cdr3a)))\n",
    "    output_0 = keras.layers.Dense(400, activation='sigmoid')(input_0)\n",
    "    # > layer for cdr3b alone\n",
    "    input_1 = keras.layers.Input(shape=(len(cols_cdr3b)))\n",
    "    output_1 = keras.layers.Dense(400, activation='sigmoid')(input_1)\n",
    "    # > layer for ag alone\n",
    "    input_2 = keras.layers.Input(shape=(len(cols_ag)))\n",
    "    output_2 = keras.layers.Dense(200, activation='sigmoid')(input_2)\n",
    "    # > combined layer for tcra and tcrb\n",
    "    concat_3 = keras.layers.Concatenate()([output_0, output_1])\n",
    "    output_3 = keras.layers.Dense(300, activation='sigmoid')(concat_3)\n",
    "    # > combined layer for tcr and ag\n",
    "    concat_4 = keras.layers.Concatenate()([output_2, output_3])\n",
    "    output_4 = keras.layers.Dense(200, activation='sigmoid')(concat_4)\n",
    "    # > final logit softmax layer\n",
    "    output_5 = keras.layers.Dense(1, activation='sigmoid')(output_4)\n",
    "    model = keras.Model(inputs=[input_0, input_1, input_2], outputs=[output_5])\n",
    "    # set up the training parameters for the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC'],\n",
    "    )\n",
    "    # train the model\n",
    "    history = model.fit([X_train[cols_cdr3a], X_train[cols_cdr3b], X_train[cols_ag]], y_train,\n",
    "                        epochs=10,\n",
    "                        validation_data=([X_train[cols_cdr3a], X_train[cols_cdr3b], X_train[cols_ag]], y_train),\n",
    "                        workers=40, use_multiprocessing=True)\n",
    "\n",
    "    # testing predictions\n",
    "    y_pred = model.predict([X_test[cols_cdr3a], X_test[cols_cdr3b], X_test[cols_ag]],\n",
    "                           workers=40, use_multiprocessing=True)[:, 0]\n",
    "    y_pred = pd.Series(y_pred, index=test.index, name='Pred%')\n",
    "\n",
    "    hla2pred[hla] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fbffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back in predictions\n",
    "test = test_orig.copy()\n",
    "test['Prediction'] = np.nan\n",
    "for pred in hla2pred.values():\n",
    "    test.loc[pred.index, 'Prediction'] = pred\n",
    "# write down predictions\n",
    "test.to_csv('../external/tcr-specificity-prediction-challenge/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dc95f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd2551e5",
   "metadata": {},
   "source": [
    "## tcra and tcrb only v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d896680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve hla specific predictions\n",
    "hla2pred = {}\n",
    "for hla in test_orig['HLA'].unique():\n",
    "    # subset for HLA\n",
    "    train = train_orig.loc[train_orig['HLA'] == hla].copy()\n",
    "    test = test_orig.loc[test_orig['HLA'] == hla].copy()\n",
    "    train = train.dropna(subset=['CDR3a_extended'])\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    # set seed and identify irrelevant matches\n",
    "    np.random.seed(0)\n",
    "    irrs = []\n",
    "\n",
    "    # re count the peptides for this HLA\n",
    "    counts = train['Peptide'].value_counts()\n",
    "    for pep in tqdm(counts.index):\n",
    "        # gather peptide cdr3 information\n",
    "        n_cdr3s = counts.loc[pep]\n",
    "        pep_cdr3s = train.loc[train['Peptide'] == pep, ['CDR3a_extended','CDR3b_extended']].value_counts().reset_index()\n",
    "        # we first grab the peptide levenshtein distances\n",
    "        pep_levenshtein = train_l[pep].sort_values()[::-1]\n",
    "        # systematically identify negative controls\n",
    "        irr_cdr3s = []\n",
    "        while n_cdr3s > 0:\n",
    "            # then we look at the max distance, gather those peptides, randomly choose one\n",
    "            # find associated CDR3s that don't overlap with the current peptide\n",
    "            vmax = pep_levenshtein.max()\n",
    "            irr_peps = pep_levenshtein.index[pep_levenshtein == vmax]\n",
    "            # reset for the next round if needed\n",
    "            pep_levenshtein = pep_levenshtein[pep_levenshtein < vmax]\n",
    "            # find the irrelevant peptide CDR3s and make sure they don't overlap\n",
    "            mask = (train_orig['Peptide'].isin(irr_peps)) &\\\n",
    "            ((~train_orig['CDR3a_extended'].isin(pep_cdr3s['CDR3a_extended'])) |\\\n",
    "            (~train_orig['CDR3b_extended'].isin(pep_cdr3s['CDR3b_extended']))) &\\\n",
    "            (~train_orig['CDR3a_extended'].isna()) &\\\n",
    "            (~train_orig['CDR3b_extended'].isna())\n",
    "            if sum(mask) == 0:\n",
    "                continue\n",
    "            # if we have cdr3s then grab them out\n",
    "            cdr3s = train_orig.loc[mask, ['CDR3a_extended','CDR3b_extended']]\n",
    "            # if there are more than we need randomly select\n",
    "            if len(cdr3s) > n_cdr3s:\n",
    "                irr_cdr3s_idxs = np.random.choice(cdr3s.index, size=n_cdr3s, replace=False)\n",
    "                irr_cdr3s += cdr3s.loc[irr_cdr3s_idxs].values.tolist()\n",
    "                break\n",
    "            # otherwise keep moving\n",
    "            else:\n",
    "                irr_cdr3s += cdr3s.values.tolist()\n",
    "                n_cdr3s -= len(cdr3s)\n",
    "        # compile the full list\n",
    "        irr = pd.DataFrame(irr_cdr3s, columns=['CDR3a_extended','CDR3b_extended'])\n",
    "        irr['Peptide'] = pep\n",
    "        irrs.append(irr)\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "    ## CONVERT TO CORRECT FORMAT\n",
    "    # create X for training\n",
    "    X_train = pd.concat([pd.concat(irrs, axis=0), train], axis=0)\n",
    "    X_train_cdr3as = cdr3a_to_X.loc[X_train['CDR3a_extended']].reset_index().iloc[:, 1:]\n",
    "    X_train_cdr3bs = cdr3b_to_X.loc[X_train['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_train_epitopes = ag_to_X.loc[X_train['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_train_cdr3as.columns = 'cdr3a:' + X_train_cdr3as.columns\n",
    "    X_train_cdr3bs.columns = 'cdr3b:' + X_train_cdr3bs.columns\n",
    "    X_train_epitopes.columns = 'ag:' + X_train_epitopes.columns\n",
    "    X_train = X_train_cdr3as.join(X_train_cdr3bs).join(X_train_epitopes)\n",
    "\n",
    "    # grab y for training\n",
    "    y_train = pd.Series(0, index=X_train.index)\n",
    "    y_train.iloc[-train.shape[0]:] = 1\n",
    "\n",
    "    # confirm the same length\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "    # create X for testing\n",
    "    X_test_cdr3as = cdr3a_to_X.loc[test['CDR3a_extended']].reset_index().iloc[:, 1:]\n",
    "    X_test_cdr3bs = cdr3b_to_X.loc[test['CDR3b_extended']].reset_index().iloc[:, 1:]\n",
    "    X_test_epitopes = ag_to_X.loc[test['Peptide']].reset_index().iloc[:, 1:]\n",
    "    # combine\n",
    "    X_test_cdr3as.columns = 'cdr3a:' + X_test_cdr3as.columns\n",
    "    X_test_cdr3bs.columns = 'cdr3b:' + X_test_cdr3bs.columns\n",
    "    X_test_epitopes.columns = 'ag:' + X_test_epitopes.columns\n",
    "    X_test = X_test_cdr3as.join(X_test_cdr3bs).join(X_test_epitopes)\n",
    "\n",
    "    # remove constant columns\n",
    "    X_train = X_train.loc[:, X_train.nunique(0) > 1]\n",
    "    X_train = X_train.loc[:, X_train.sum(0) > 0]\n",
    "    # read in normalization factors\n",
    "    means = X_train.mean(0)\n",
    "    stds = (X_train - means).std(0)\n",
    "    # subset for relevant columns\n",
    "    X_train = X_train[means.index]\n",
    "    X_test = X_test[means.index]\n",
    "    # normalize\n",
    "    X_train -= means\n",
    "    X_train /= stds\n",
    "    X_test -= means\n",
    "    X_test /= stds\n",
    "\n",
    "\n",
    "    ## SETUP MODEL\n",
    "    # retrieve the appropriate columns\n",
    "    cols_cdr3a = X_train.columns[X_train.columns.str.startswith('cdr3a')]\n",
    "    cols_cdr3b = X_train.columns[X_train.columns.str.startswith('cdr3b')]\n",
    "    cols_ag = X_train.columns[X_train.columns.str.startswith('ag')]\n",
    "\n",
    "    # determine model parameters\n",
    "    # > layer for cdr3a alone\n",
    "    input_0 = keras.layers.Input(shape=(len(cols_cdr3a)))\n",
    "    output_0 = keras.layers.Dense(1000, activation='sigmoid')(input_0)\n",
    "    # > layer for cdr3b alone\n",
    "    input_1 = keras.layers.Input(shape=(len(cols_cdr3b)))\n",
    "    output_1 = keras.layers.Dense(1000, activation='sigmoid')(input_1)\n",
    "    # > layer for ag alone\n",
    "    input_2 = keras.layers.Input(shape=(len(cols_ag)))\n",
    "    output_2 = keras.layers.Dense(500, activation='sigmoid')(input_2)\n",
    "    # > combined layer for tcra and tcrb\n",
    "    concat_3 = keras.layers.Concatenate()([output_0, output_1])\n",
    "    output_3 = keras.layers.Dense(1000, activation='sigmoid')(concat_3)\n",
    "    # > combined layer for tcr and ag\n",
    "    concat_4 = keras.layers.Concatenate()([output_2, output_3])\n",
    "    output_4 = keras.layers.Dense(500, activation='sigmoid')(concat_4)\n",
    "    # > final logit softmax layer\n",
    "    output_5 = keras.layers.Dense(1, activation='sigmoid')(output_4)\n",
    "    model = keras.Model(inputs=[input_0, input_1, input_2], outputs=[output_5])\n",
    "    # set up the training parameters for the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC'],\n",
    "    )\n",
    "    # train the model\n",
    "    history = model.fit([X_train[cols_cdr3a], X_train[cols_cdr3b], X_train[cols_ag]], y_train,\n",
    "                        epochs=10,\n",
    "                        validation_data=([X_train[cols_cdr3a], X_train[cols_cdr3b], X_train[cols_ag]], y_train),\n",
    "                        workers=40, use_multiprocessing=True)\n",
    "\n",
    "    # testing predictions\n",
    "    y_pred = model.predict([X_test[cols_cdr3a], X_test[cols_cdr3b], X_test[cols_ag]],\n",
    "                           workers=40, use_multiprocessing=True)[:, 0]\n",
    "    y_pred = pd.Series(y_pred, index=test.index, name='Pred%')\n",
    "\n",
    "    hla2pred[hla] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back in predictions\n",
    "test = test_orig.copy()\n",
    "test['Prediction'] = np.nan\n",
    "for pred in hla2pred.values():\n",
    "    test.loc[pred.index, 'Prediction'] = pred\n",
    "# write down predictions\n",
    "test.to_csv('../external/tcr-specificity-prediction-challenge/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ec33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5de05e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d1ca05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c89fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "# define inputs with scores from immrep\n",
    "df = pd.DataFrame([['TCRb','a',.6731],['TCRb','b',.6801],['TCRb','c',.67686131],\n",
    "                   ['TCRb','d',.6891],['TCRb','e',.6819],\n",
    "                   ['TCRa+b','a',.7583],['TCRa+b','b',.7635],['TCRa+b','c',.7621],\n",
    "                   ['TCRa+b','d',.7360],['TCRa+b','e',.7539],], columns=['input','round','score'])\n",
    "# create plot, set specific seed for jitter reproducibility\n",
    "np.random.seed(27)\n",
    "fig, ax = plt.subplots(figsize=[2, 4])\n",
    "ax.grid(False)\n",
    "sns.stripplot(x='input', y='score', hue='round', data=df, s=10, jitter=0.25)\n",
    "ax.legend('', frameon=False)\n",
    "# plot the average lines of each\n",
    "mean_b = df.loc[df['input'] == 'TCRb', 'score'].mean()\n",
    "mean_ab = df.loc[df['input'] == 'TCRa+b', 'score'].mean()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "ax.plot(xlim, [mean_b]*2, linestyle='--', color='k')\n",
    "ax.plot(xlim, [mean_ab]*2, linestyle='--', color='k')\n",
    "ax.set_xlim(*xlim)\n",
    "ax.set_ylim(ylim[0], ylim[1]+0.02)\n",
    "# derive the p-value\n",
    "p = ss.mannwhitneyu(df.loc[df['input'] == 'TCRb', 'score'], df.loc[df['input'] == 'TCRa+b', 'score'])[1]\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f237d4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c0b812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c68db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa1d732b",
   "metadata": {},
   "source": [
    "## fully upgraded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433fc50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE CONSTANTS\n",
    "stretch_length_cdr3 = 30\n",
    "stretch_length_ag = 15\n",
    "aa_alphabet = ['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']\n",
    "bcps = ['charge','hydrophobicity','weight','sulfur','aromatic']\n",
    "\n",
    "## DEFINE ENCODERS\n",
    "def _encode_peptide(peptide):\n",
    "    # define columns\n",
    "    idxs = range(len(peptide))\n",
    "    cols = aa_alphabet+bcps\n",
    "    # create the tracking dataframe\n",
    "    X = pd.DataFrame(np.nan, index=idxs, columns=cols)\n",
    "    for idx, aa in enumerate(peptide):\n",
    "        # map to each of the bio-chem-physical components\n",
    "        charge = amino_acid_charge[aa]\n",
    "        hydrophobicity = amino_acid_hydrophobicity[aa] / 2\n",
    "        weight = amino_acid_weights[aa] / stretch_length_cdr3\n",
    "        sulfur = 1 * (aa in has_sulfur)\n",
    "        aromatic = 1 * (aa in is_aromatic)\n",
    "        # store in the final dataframe\n",
    "        X.loc[idx, aa_alphabet] = 0\n",
    "        X.loc[idx, aa] = 1\n",
    "        X.loc[idx, bcps] = charge, hydrophobicity, weight, sulfur, aromatic\n",
    "    assert not X.isna().any().any()\n",
    "    return {peptide:X}\n",
    "def encode_peptide(peptides, n_cpus):\n",
    "    # work through peptides\n",
    "    peptide_to_X = {}\n",
    "    with mp.Pool(n_cpus) as pool:\n",
    "        for result in pool.imap_unordered(_encode_peptide, peptides):\n",
    "            peptide_to_X.update(result)\n",
    "    return peptide_to_X\n",
    "\n",
    "# retrieve encodings\n",
    "cdr3a_to_X2 = encode_peptide(pd.concat([train_orig, test_orig], axis=0)['CDR3a_extended'].dropna().unique(), 40)\n",
    "cdr3b_to_X2 = encode_peptide(pd.concat([train_orig, test_orig], axis=0)['CDR3b_extended'].dropna().unique(), 40)\n",
    "ag_to_X2 = encode_peptide(pd.concat([train_orig, test_orig], axis=0)['Peptide'].dropna().unique(), 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603f83b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve hla specific training and testing data\n",
    "hla2data2 = {}\n",
    "for hla in test_orig['HLA'].unique():\n",
    "    # subset for HLA\n",
    "    train = train_orig.loc[train_orig['HLA'] == hla].copy()\n",
    "    test = test_orig.loc[test_orig['HLA'] == hla].copy()\n",
    "    train = train.dropna(subset=['CDR3a_extended'])\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    # set seed and identify irrelevant matches\n",
    "    np.random.seed(0)\n",
    "    irrs = []\n",
    "\n",
    "    # re count the peptides for this HLA\n",
    "    counts = train['Peptide'].value_counts()\n",
    "    for pep in tqdm(counts.index):\n",
    "        # gather peptide cdr3 information\n",
    "        n_cdr3s = counts.loc[pep]\n",
    "        pep_cdr3s = train.loc[train['Peptide'] == pep, ['CDR3a_extended','CDR3b_extended']].value_counts().reset_index()\n",
    "        # we first grab the peptide levenshtein distances\n",
    "        pep_levenshtein = train_l[pep].sort_values()[::-1]\n",
    "        # systematically identify negative controls\n",
    "        irr_cdr3s = []\n",
    "        while n_cdr3s > 0:\n",
    "            # then we look at the max distance, gather those peptides, randomly choose one\n",
    "            # find associated CDR3s that don't overlap with the current peptide\n",
    "            vmax = pep_levenshtein.max()\n",
    "            irr_peps = pep_levenshtein.index[pep_levenshtein == vmax]\n",
    "            # reset for the next round if needed\n",
    "            pep_levenshtein = pep_levenshtein[pep_levenshtein < vmax]\n",
    "            # find the irrelevant peptide CDR3s and make sure they don't overlap\n",
    "            mask = (train_orig['Peptide'].isin(irr_peps)) &\\\n",
    "            ((~train_orig['CDR3a_extended'].isin(pep_cdr3s['CDR3a_extended'])) |\\\n",
    "            (~train_orig['CDR3b_extended'].isin(pep_cdr3s['CDR3b_extended']))) &\\\n",
    "            (~train_orig['CDR3a_extended'].isna()) &\\\n",
    "            (~train_orig['CDR3b_extended'].isna())\n",
    "            if sum(mask) == 0:\n",
    "                continue\n",
    "            # if we have cdr3s then grab them out\n",
    "            cdr3s = train_orig.loc[mask, ['CDR3a_extended','CDR3b_extended']]\n",
    "            # if there are more than we need randomly select\n",
    "            if len(cdr3s) > n_cdr3s:\n",
    "                irr_cdr3s_idxs = np.random.choice(cdr3s.index, size=n_cdr3s, replace=False)\n",
    "                irr_cdr3s += cdr3s.loc[irr_cdr3s_idxs].values.tolist()\n",
    "                break\n",
    "            # otherwise keep moving\n",
    "            else:\n",
    "                irr_cdr3s += cdr3s.values.tolist()\n",
    "                n_cdr3s -= len(cdr3s)\n",
    "        # compile the full list\n",
    "        irr = pd.DataFrame(irr_cdr3s, columns=['CDR3a_extended','CDR3b_extended'])\n",
    "        irr['Peptide'] = pep\n",
    "        irrs.append(irr)\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "    ## CONVERT TO CORRECT FORMAT\n",
    "    # create X for training\n",
    "    X_train = pd.concat([pd.concat(irrs, axis=0), train], axis=0).reset_index().iloc[:, 1:]\n",
    "    X_train_cdr3as, X_train_cdr3bs, X_train_ags = [], [], []\n",
    "    for idx in X_train.index:\n",
    "        # retrieve the data\n",
    "        cdr3a, cdr3b, ag = X_train.loc[idx, 'CDR3a_extended'], X_train.loc[idx, 'CDR3b_extended'], X_train.loc[idx, 'Peptide']\n",
    "        # grab the dataframes associated with the data\n",
    "        X_tra, X_trb, X_ag = cdr3a_to_X2[cdr3a], cdr3b_to_X2[cdr3b], ag_to_X2[ag]\n",
    "        # interpolate out to n=30 (no padding)\n",
    "        X_tra2 = pd.DataFrame(index=range(stretch_length_cdr3), columns=X_tra.columns)\n",
    "        for col in X_tra2.columns:\n",
    "            X_tra2[col] = np.interp(range(stretch_length_cdr3), range(X_tra.shape[0]), X_tra[col])\n",
    "        X_trb2 = pd.DataFrame(index=range(stretch_length_cdr3), columns=X_trb.columns)\n",
    "        for col in X_trb2.columns:\n",
    "            X_trb2[col] = np.interp(range(stretch_length_cdr3), range(X_trb.shape[0]), X_trb[col])\n",
    "        # interpolate out to n=15 (no padding)\n",
    "        X_ag2 = pd.DataFrame(index=range(stretch_length_ag), columns=X_ag.columns)\n",
    "        for col in X_tra2.columns:\n",
    "            X_ag2[col] = np.interp(range(stretch_length_ag), range(X_ag.shape[0]), X_ag[col])\n",
    "        # append\n",
    "        X_train_cdr3as.append(X_tra2)\n",
    "        X_train_cdr3bs.append(X_trb2)\n",
    "        X_train_ags.append(X_ag2)\n",
    "    # grab y for training\n",
    "    y_train = pd.Series(0, index=X_train.index)\n",
    "    y_train.iloc[-train.shape[0]:] = 1\n",
    "    # confirm the same length\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "    # create X for testing\n",
    "    X_test = test.copy()\n",
    "    X_test_cdr3as, X_test_cdr3bs, X_test_ags = [], [], []\n",
    "    for idx in X_test.index:\n",
    "        # retrieve the data\n",
    "        cdr3a, cdr3b, ag = X_test.loc[idx, 'CDR3a_extended'], X_test.loc[idx, 'CDR3b_extended'], X_test.loc[idx, 'Peptide']\n",
    "        # grab the dataframes associated with the data\n",
    "        X_tra, X_trb, X_ag = cdr3a_to_X2[cdr3a], cdr3b_to_X2[cdr3b], ag_to_X2[ag]\n",
    "        # interpolate out to n=30 (no padding)\n",
    "        X_tra2 = pd.DataFrame(index=range(stretch_length_cdr3), columns=X_tra.columns)\n",
    "        for col in X_tra2.columns:\n",
    "            X_tra2[col] = np.interp(range(stretch_length_cdr3), range(X_tra.shape[0]), X_tra[col])\n",
    "        X_trb2 = pd.DataFrame(index=range(stretch_length_cdr3), columns=X_trb.columns)\n",
    "        for col in X_trb2.columns:\n",
    "            X_trb2[col] = np.interp(range(stretch_length_cdr3), range(X_trb.shape[0]), X_trb[col])\n",
    "        # interpolate out to n=15 (no padding)\n",
    "        X_ag2 = pd.DataFrame(index=range(stretch_length_ag), columns=X_ag.columns)\n",
    "        for col in X_tra2.columns:\n",
    "            X_ag2[col] = np.interp(range(stretch_length_ag), range(X_ag.shape[0]), X_ag[col])\n",
    "        # append\n",
    "        X_test_cdr3as.append(X_tra2)\n",
    "        X_test_cdr3bs.append(X_trb2)\n",
    "        X_test_ags.append(X_ag2)\n",
    "\n",
    "    # transform all into numpy arrays\n",
    "    X_train_cdr3as, X_train_cdr3bs, X_train_ags = np.array(X_train_cdr3as), np.array(X_train_cdr3bs), np.array(X_train_ags)\n",
    "    X_test_cdr3as, X_test_cdr3bs, X_test_ags = np.array(X_test_cdr3as), np.array(X_test_cdr3bs), np.array(X_test_ags)\n",
    "    hla2data2[hla] = X_train_cdr3as, X_train_cdr3bs, X_train_ags, X_test_cdr3as, X_test_cdr3bs, X_test_ags, y_train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcfd59d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve hla specific predictions\n",
    "hla2pred = {}\n",
    "for hla in test_orig['HLA'].unique():\n",
    "    # set seed\n",
    "    np.random.seed(0)\n",
    "    # read in all into numpy arrays\n",
    "    X_train_cdr3as, X_train_cdr3bs, X_train_ags, X_test_cdr3as, X_test_cdr3bs, X_test_ags, y_train, test = hla2data2[hla]\n",
    "\n",
    "    ## SETUP MODEL\n",
    "    # > layer for tra\n",
    "    input_1 = keras.layers.Input(shape=(stretch_length_cdr3, len(aa_alphabet)+len(bcps)))\n",
    "    output_1 = keras.layers.Conv1D(30, 3, activation='gelu')(input_1)\n",
    "    flatten_1a = keras.layers.Flatten()(output_1)\n",
    "    flatten_1b = keras.layers.Flatten()(input_1)\n",
    "    concat_1 = keras.layers.Concatenate()([flatten_1a, flatten_1b])\n",
    "    output_1 = keras.layers.Dense(300, activation='sigmoid')(concat_1)\n",
    "    # > layer for trb\n",
    "    input_2 = keras.layers.Input(shape=(stretch_length_cdr3, len(aa_alphabet)+len(bcps)))\n",
    "    output_2 = keras.layers.Conv1D(30, 3, activation='gelu')(input_2)\n",
    "    flatten_2a = keras.layers.Flatten()(output_2)\n",
    "    flatten_2b = keras.layers.Flatten()(input_2)\n",
    "    concat_2 = keras.layers.Concatenate()([flatten_2a, flatten_2b])\n",
    "    output_2 = keras.layers.Dense(300, activation='sigmoid')(concat_2)\n",
    "    # > layer for ag\n",
    "    input_3 = keras.layers.Input(shape=(stretch_length_ag, len(aa_alphabet)+len(bcps)))\n",
    "    output_3 = keras.layers.Conv1D(15, 3, activation='gelu')(input_2)\n",
    "    flatten_3a = keras.layers.Flatten()(output_3)\n",
    "    flatten_3b = keras.layers.Flatten()(input_3)\n",
    "    concat_3 = keras.layers.Concatenate()([flatten_3a, flatten_3b])\n",
    "    output_3 = keras.layers.Dense(150, activation='sigmoid')(concat_3)\n",
    "    # > concat tra + ag\n",
    "    concat_13 = keras.layers.Concatenate()([output_1, output_3])\n",
    "    output_13 = keras.layers.Dense(100, activation='sigmoid')(concat_13)\n",
    "    # > concat trb + ag\n",
    "    concat_23 = keras.layers.Concatenate()([output_2, output_3])\n",
    "    output_23 = keras.layers.Dense(100, activation='sigmoid')(concat_23)\n",
    "    # > concat tra + trb\n",
    "    concat_12 = keras.layers.Concatenate()([output_1, output_2])\n",
    "    output_12 = keras.layers.Dense(100, activation='sigmoid')(concat_12)\n",
    "    # > concat tra+trb + ag\n",
    "    concat_123 = keras.layers.Concatenate()([output_12, output_3])\n",
    "    output_123 = keras.layers.Dense(100, activation='sigmoid')(concat_123)\n",
    "    # > full combination\n",
    "    concat_full = keras.layers.Concatenate()([output_12, output_13, output_123])\n",
    "    output_full = keras.layers.Dense(1, activation='sigmoid')(concat_full)\n",
    "    model = keras.Model(inputs=[input_1, input_2, input_3], outputs=[output_full])\n",
    "\n",
    "    # set up the training parameters for the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy','AUC'],\n",
    "    )\n",
    "    # train the model\n",
    "    history = model.fit([X_train_cdr3as, X_train_cdr3bs, X_train_ags], y_train,\n",
    "                        epochs=10,\n",
    "                        validation_data=([X_train_cdr3as, X_train_cdr3bs, X_train_ags], y_train),\n",
    "                        workers=40, use_multiprocessing=True)\n",
    "\n",
    "    # testing predictions\n",
    "    y_pred = model.predict([X_test_cdr3as, X_test_cdr3bs, X_test_ags],\n",
    "                           workers=40, use_multiprocessing=True)[:, 0]\n",
    "    y_pred = pd.Series(y_pred, index=test.index, name='Pred%')\n",
    "    hla2pred[hla] = y_pred\n",
    "\n",
    "# add back in predictions\n",
    "test = test_orig.copy()\n",
    "test['Prediction'] = np.nan\n",
    "for pred in hla2pred.values():\n",
    "    test.loc[pred.index, 'Prediction'] = pred\n",
    "# write down predictions\n",
    "test.to_csv('/ssd1/dchen/TMP/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0894a22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3450920f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24000003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
